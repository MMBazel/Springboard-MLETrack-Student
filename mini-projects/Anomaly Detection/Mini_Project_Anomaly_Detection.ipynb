{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Detection of Retail Store Sales\n",
    "\n",
    "This hands-on mini-project will enable you to reinforce your learnings pertaining to anomaly detection in this unit. By now, you must already be aware of the key objective of anomaly detection. Just to refresh your memory, anomaly detection is the identification of outliers or rare event items in a dataset which potentially exhibit abnormal behavior or properties as compared to the rest of the datapoints.\n",
    "\n",
    "There are a wide variety of anomaly detection methods including supervised, unsupervised and semi-supervised. Typically you can perform anomaly detection on univariate data, multivariate data as well as data which is temporal in nature. In this mini-project you will leverage state-of-the-art anomaly detection models from frameworks like [__`scikit-learn`__](https://scikit-learn.org/stable/modules/outlier_detection.html) and [__`PyOD`__](https://pyod.readthedocs.io/en/latest/index.html).\n",
    "\n",
    "\n",
    "By the end of this mini-project, you will have successfully applied these techniques to find out potential outliers pertaining to sales transactional data in a retail store dataset and also learnt how to visualize outliers similar to the following plot.\n",
    "\n",
    "![](outlier_ex.png)\n",
    "\n",
    "We will be performing anomaly detection on both univariate and multivariate data and leverage the following anomaly detection techniques.\n",
    "\n",
    "- Simple Statistical Models (mean & standard deviation: the three-sigma rule)\n",
    "- Isolation Forest\n",
    "- Clustering-Based Local Outlier Factor\n",
    "- Auto-encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Getting and Loading the Dataset\n",
    "\n",
    "The first step towards solving any data science or machine learning problem is to obtain the necessary data. In this scenario, we will be dealing with a popular retail dataset known as the [SuperStore Sales Dataset](https://community.tableau.com/docs/DOC-1236) which consists of transactional data pertaining to a retail store.\n",
    "\n",
    "#### Please download the required dataset from [here](https://community.tableau.com/docs/DOC-1236) if necessary, although it will also be provided to you along with this notebook for this mini-project\n",
    "\n",
    "Once we have the necessary data, we will load up the dataset and perform some initial exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Exploratory Data Analysis\n",
    "\n",
    "It's time to do some basic exploratory analysis on the retail store transactional data. We start by loading up the dataset into a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "df = pd.read_excel(\"./Superstore.xls\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't have any major missing values in our dataset and we can now look at a sample subset of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Sales vs. Order Date \n",
    "\n",
    "Let's look more closely at the __`Sales`__ attribute of the dataset in the next few cells. We'll start by looking at typical sales over time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "sns.lineplot(x=df['Order Date'], y=df['Sales']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Sales Distribution\n",
    "\n",
    "\n",
    "Let's now look at the data distribution for __`Sales`__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df['Sales'])\n",
    "plt.title(\"Sales Distribution\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Sales'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can definitely see the presence of potential outliers in terms of the min or max values as compared to the meat of the distribution in the interquartile range as observed in the distribution statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q 2.1: Visualize Profit vs. Order Date \n",
    "\n",
    "Let's now look closely at the __`Profit`__ attribute of the dataset in the next few cells. We'll start by looking at typical profits over time.\n",
    "\n",
    "__Your turn: Plot `Order Date` vs. `Profit` using a line plot__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<FILL BLANKS HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q 2.2: Visualize Profit Distribution\n",
    "\n",
    "Let's now look at the data distribution for __`Profit`__\n",
    "\n",
    "__Your turn: Plot the distribution for `Profit`__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<FILL BLANKS HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Your turn: Get the essential descriptive statistics for `Profit` using an appropriate function__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<FILL BLANKS HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Your turn: Do you notice anything interesting about the distribution?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have both positive and negative values in profits since it indicates either a profit or a loss based on the sales and original price of the items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Discount vs. Profit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=\"Discount\", y=\"Profit\", data=df);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above visual, we look at a scatter plot showing the distribution of profits w.r.t discounts given"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Univariate Anomaly Detection\n",
    "\n",
    "Univariate is basically analysis done on a single attribute or feature. In this section, we will perform anomaly detection on a single attribute using the following methods.\n",
    "\n",
    "- Statistical Process Control Methods (mean + 3sigma thresholding)\n",
    "- Isolation Forest\n",
    "\n",
    "We will start off by demonstrating both these techniques on the __`Sales`__ attribute and later on, you will implement similar techniques on the __`Profit`__ attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1: Univariate Anomaly Detection on Sales using Statistical Modeling\n",
    "\n",
    "Here we start off by implementing anomaly detecting using statistical modeling on the __`Sales`__ attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain Upper Limit Threshold for Sales\n",
    "\n",
    "Here we are concerned about transactions with high sales values so we compute the upper limit using the $\\mu$ + 3$\\sigma$ rule where $\\mu$ is the mean of the distribution and $\\sigma$ is the standard deviation of the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_sales = df['Sales'].mean()\n",
    "sigma_sales = df['Sales'].std()\n",
    "three_sigma_sales = 3*sigma_sales\n",
    "\n",
    "threshold_sales_value = mean_sales + three_sigma_sales\n",
    "print('Threshold Sales:', threshold_sales_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Outlier Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "\n",
    "sns.distplot(df['Sales'])\n",
    "plt.axvspan(threshold_sales_value, df['Sales'].max(), facecolor='r', alpha=0.3)\n",
    "plt.title(\"Sales Distribution with Outlier Region\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter and Sort Outliers\n",
    "\n",
    "Here we filter out the outlier observations and sort by descending order and view the top 5 outlier values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_outliers_df = df['Sales'][df['Sales'] > threshold_sales_value]\n",
    "print('Total Sales Outliers:', len(sales_outliers_df))\n",
    "sales_outliers_sorted = sales_outliers_df.sort_values(ascending=False)\n",
    "sales_outliers_sorted.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Top 10 Outlier Transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.loc[sales_outliers_sorted.index.tolist()][['City', 'Category', 'Sub-Category', 'Product Name', \n",
    "                                              'Sales', 'Quantity', 'Discount', 'Profit']]).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Bottom 10 Outlier Transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.loc[sales_outliers_sorted.index.tolist()][['City', 'Category', 'Sub-Category', 'Product Name', \n",
    "                                              'Sales', 'Quantity', 'Discount', 'Profit']]).tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q 3.2: Univariate Anomaly Detection on Profit using Statistical Modeling\n",
    "\n",
    "In this section you will use the learning from Section 3.1 and implement anomaly detecting using statistical modeling on the __`Profit`__ attribute. Since we have both +ve (profits) and -ve (losses) values in the distribution, we will try to find anomalies for each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain Upper Limit Threshold for Profit\n",
    "\n",
    "__Your turn:__ Compute the upper and lower limits using the  𝜇  + 3 𝜎  rule where  𝜇  is the mean of the distribution and  𝜎  is the standard deviation of the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_profit = #<FILL THIS>\n",
    "sigma_profit = #<FILL THIS>\n",
    "three_sigma_profit = #<FILL THIS>\n",
    "\n",
    "threshold_profit_upper_limit = #<FILL THIS>\n",
    "threshold_profit_lower_limit = #<FILL THIS>\n",
    "\n",
    "print('Thresholds Profit:', threshold_profit_lower_limit, threshold_profit_upper_limit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Outlier Regions\n",
    "\n",
    "__Your turn:__ Visualize the upper and lower outlier regions in the distribution similar to what you did in 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<FILL BLANKS HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter and Sort Outliers\n",
    "\n",
    "__Your turn:__ Filter out the outlier observations and sort by descending order and view the top 5 outlier values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<FILL BLANKS HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Top 10 Outlier Transactions\n",
    "\n",
    "__Your turn:__ View the top ten transactions based on highest profits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<FILL BLANKS HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q: Do you notice any interesting insights based on these transactions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A:__ Most of these are purchases for Copiers and Binders , looks like Canon products yielded some good profits`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Bottom 10 Outlier Transactions\n",
    "\n",
    "__Your turn:__ View the bottom ten transactions based on lowest profits (highest losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<FILL BLANKS HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q: Do you notice any interesting insights based on these transactions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A:__ Most of these are purchases for Machines and Binders , looks like Cibify 3D Printers yielded high losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3: Univariate Anomaly Detection on Sales using Isolation Forest\n",
    "\n",
    "You might have already learnt about this model from the curriculum. Just to briefly recap, the Isolation Forest model,  'isolates' observations by randomly selecting a feature and then randomly selecting a split value between the maximum and minimum values of the selected feature.\n",
    "\n",
    "Recursive partitioning can be represented by a tree structure. Hence, the number of splittings required to isolate a sample is equivalent to the path length from the root node to the terminating node. This path length, averaged over a forest of such random trees, is a measure of normality and our decision function.\n",
    "\n",
    "Random partitioning produces noticeably shorter paths for anomalies. Hence, when a forest of random trees collectively produce shorter path lengths for particular samples, they are highly likely to be anomalies.\n",
    "\n",
    "More details are available in this [User Guide](https://scikit-learn.org/stable/modules/outlier_detection.html#isolation-forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize and Train Model\n",
    "\n",
    "Here we initialize the isolation forest model with some hyperparameters assuming the proportion of outliers to be 1% of the total data (using the `contamination` setting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "sales_ifmodel = IsolationForest(n_estimators=100,\n",
    "                                contamination=0.01)\n",
    "sales_ifmodel.fit(df[['Sales']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Outlier Region\n",
    "\n",
    "Here we visualize the outlier region in the data distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "xx = np.linspace(df['Sales'].min(), df['Sales'].max(), len(df)).reshape(-1,1)\n",
    "anomaly_score = sales_ifmodel.decision_function(xx)\n",
    "outlier = sales_ifmodel.predict(xx)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(xx, anomaly_score, label='anomaly score')\n",
    "plt.fill_between(xx.T[0], np.min(anomaly_score), np.max(anomaly_score), \n",
    "                 where=outlier==-1, color='r', \n",
    "                 alpha=.4, label='outlier region')\n",
    "plt.legend()\n",
    "plt.ylabel('anomaly score')\n",
    "plt.xlabel('Sales');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter and Sort Outliers\n",
    "\n",
    "Here we predict outliers in our dataset using our trained model and filter out the outlier observations and sort by descending order and view the top 5 outlier values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_predictions = sales_ifmodel.predict(df[['Sales']])\n",
    "\n",
    "sales_outliers_df = df[['Sales']]\n",
    "sales_outliers_df['Outlier'] = outlier_predictions\n",
    "sales_outliers_df = sales_outliers_df[sales_outliers_df['Outlier'] == -1]['Sales']\n",
    "\n",
    "print('Total Sales Outliers:', len(sales_outliers_df))\n",
    "sales_outliers_sorted = sales_outliers_df.sort_values(ascending=False)\n",
    "sales_outliers_sorted.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Top 10 Outlier Transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.loc[sales_outliers_sorted.index.tolist()][['City', 'Category', 'Sub-Category', 'Product Name', \n",
    "                                              'Sales', 'Quantity', 'Discount', 'Profit']]).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Bottom 10 Outlier Transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.loc[sales_outliers_sorted.index.tolist()][['City', 'Category', 'Sub-Category', 'Product Name', \n",
    "                                              'Sales', 'Quantity', 'Discount', 'Profit']]).tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q 3.4: Univariate Anomaly Detection on Profit using Isolation Forest\n",
    "\n",
    "In this section you will use the learning from Section 3.3 and implement anomaly detecting using isolation on the __`Profit`__ attribute. Since we have both +ve (profits) and -ve (losses) values in the distribution, we will try to find anomalies for each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize and Train Model\n",
    "\n",
    "__Your Turn:__ Initialize the isolation forest model with similar hyperparameters as Section 3.3 and also assuming the proportion of outliers to be 1% of the total data (using the contamination setting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<FILL BLANKS HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Outlier Regions\n",
    "\n",
    "__Your turn:__ Visualize the upper and lower outlier regions in the distribution similar to what you did in 3.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<FILL BLANKS HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter and Sort Outliers\n",
    "\n",
    "__Your Turn:__ Predict outliers in our dataset using our trained model and filter out the outlier observations and sort by descending order and view the top 5 outlier values similar to 3.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<FILL BLANKS HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Top 10 Outlier Transactions\n",
    "\n",
    "__Your turn:__ View the top ten transactions based on highest profits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<FILL BLANKS HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Bottom 10 Outlier Transactions\n",
    "\n",
    "__Your turn:__ View the bottom ten transactions based on lowest profits (highest losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<FILL BLANKS HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q: Do you observe any similarity in the results with the previous method?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A:__ Yes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another interesting approach to check out would be the [Generalized ESD Test for Outliers](https://www.itl.nist.gov/div898/handbook/eda/section3/eda35h3.htm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Multivariate Anomaly Detection\n",
    "\n",
    "Multivariate is basically analysis done on more than one attribute or feature at a time. In this section, we will perform anomaly detection on two attributes (__`Discount`__ & __`Profit`__) using the following methods.\n",
    "\n",
    "- Clustering Based Local Outlier Factor (CBLOF)\n",
    "- Isolation Forest\n",
    "- Auto-Encoders\n",
    "\n",
    "You will learn how to train these models to detect outliers and also visualize these outliers. For this section we will be using the __[`pyod`](https://pyod.readthedocs.io/en/latest/)__ package so make sure you have it installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Subset Data for Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['Discount', 'Profit']\n",
    "subset_df = df[cols]\n",
    "subset_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "mms = MinMaxScaler(feature_range=(0, 1))\n",
    "subset_df[cols] = mms.fit_transform(subset_df)\n",
    "subset_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1: Multivariate Anomaly Detection with Clustering Based Local Outlier Factor (CBLOF)\n",
    "\n",
    "The CBLOF model takes as an input the dataset and the cluster model that was generated by a clustering algorithm. It classifies the clusters into small clusters and large clusters using the parameters alpha and beta. The anomaly score is then calculated based on the size of the cluster the point belongs to as well as the distance to the nearest large cluster.\n",
    "\n",
    "By default, kMeans is used for clustering algorithm. You can read more in the [official documentation](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.cblof)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize and Train Model\n",
    "\n",
    "Here we initialize the CBLOF model with some hyperparameters assuming the proportion of outliers to be 1% of the total data (using the `contamination` setting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyod.models import cblof\n",
    "\n",
    "cblof_model = cblof.CBLOF(contamination=0.01, random_state=42)\n",
    "cblof_model.fit(subset_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter and Sort Outliers\n",
    "\n",
    "Here we predict outliers in our dataset using our trained model and filter out the outlier observations and sort by descending order and view the top 5 outlier values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_predictions = cblof_model.predict(subset_df)\n",
    "\n",
    "outliers_df = subset_df.copy(deep=True)\n",
    "outliers_df['Outlier'] = outlier_predictions\n",
    "outliers_df = outliers_df[outliers_df['Outlier'] == 1]\n",
    "\n",
    "print('Total Outliers:', len(outliers_df))\n",
    "outliers_sorted = outliers_df.sort_values(by=['Profit', 'Discount'], ascending=False)\n",
    "outliers_sorted.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Bottom 10 Outlier Transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.loc[outliers_sorted.index.tolist()][['City', 'Category', 'Sub-Category', 'Product Name', \n",
    "                                              'Sales', 'Quantity', 'Discount', 'Profit']]).tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can definitely see some huge losses incurred based on giving higher discounts even if the sales amount was high which is interesting as well as concerning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q 4.2: Multivariate Anomaly Detection with Isolation Forest\n",
    "\n",
    "Here you will detect anomalies using the Isolation Forest model and use the learnings from 4.1. Here you will use the [`pyod`](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.iforest) version of [Isolation Forest](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.iforest) which is basically a wrapper over the `scikit-learn` version but with more functionalities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize and Train Model\n",
    "\n",
    "__Your Turn:__ Initialize the isolation forest model with similar hyperparameters as before and also assuming the proportion of outliers to be 1% of the total data (using the contamination setting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyod.models import iforest\n",
    "\n",
    "if_model = #<FILL BLANKS HERE>\n",
    "#<FILL BLANKS HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter and Sort Outliers\n",
    "\n",
    "__Your Turn:__ Predict outliers in our dataset using our trained model and filter out the outlier observations and sort by descending order and view the top 5 outlier values similar to 4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<FILL BLANKS HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Bottom 10 Outlier Transactions\n",
    "\n",
    "__Your turn:__ View the bottom ten transactions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<FILL BLANKS HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q: Do you notice any differences in the results with the previous model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do notice some transactions with 80% discount and high losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q 4.3: Multivariate Anomaly Detection with Auto-encoders\n",
    "\n",
    "Here you will detect anomalies using the Auto-encoder model and use the learnings from 4.1. Here you will use the [Auto-encoder](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.auto_encoder) model from `pyod` which is a deep learning model often used for learning useful data representations in an unsupervised fashion without any labeled data. \n",
    "\n",
    "![](outlier_ae.png)\n",
    "\n",
    "Similar to PCA, AE could be used to detect outlier objects in the data by calculating the reconstruction errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Model\n",
    "\n",
    "Here we initiaze an auto-encoder network with a few hidden layers so that we could train it for a 100 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyod.models import auto_encoder\n",
    "\n",
    "ae_model = auto_encoder.AutoEncoder(hidden_neurons=[2, 32, 32, 2], \n",
    "                                    hidden_activation='relu',\n",
    "                                    output_activation='sigmoid',\n",
    "                                    epochs=100,\n",
    "                                    batch_size=32,\n",
    "                                    contamination=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model\n",
    "\n",
    "__Your turn:__ Train the model by calling the `fit()` function on the right data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<FILL BLANKS HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter and Sort Outliers\n",
    "\n",
    "__Your Turn:__ Predict outliers in our dataset using our trained model and filter out the outlier observations and sort by descending order and view the top 5 outlier values similar to 4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<FILL BLANKS HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Bottom 10 Outlier Transactions\n",
    "\n",
    "__Your turn:__ View the bottom ten transactions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<FILL BLANKS HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4: Visualize Anomalies and Compare Anomaly Detection Models\n",
    "\n",
    "Here we will look at the visual plots of anomalies as detected by the above three models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_anomalies(model, xx, yy, data_df, ax_obj, subplot_title):\n",
    "    \n",
    "    # predict raw anomaly score\n",
    "    scores_pred = model.decision_function(data_df) * -1\n",
    "    # prediction of a datapoint category outlier or inlier\n",
    "    y_pred = model.predict(data_df)\n",
    "    n_inliers = len(y_pred) - np.count_nonzero(y_pred)\n",
    "    n_outliers = np.count_nonzero(y_pred == 1)\n",
    "\n",
    "\n",
    "    out_df = data_df.copy(deep=True)\n",
    "    out_df['Outlier'] = y_pred.tolist()\n",
    "    # discount - inlier feature 1,  profit - inlier feature 2\n",
    "    inliers_discount = out_df[out_df['Outlier'] == 0]['Discount'].values\n",
    "    inliers_profit = out_df[out_df['Outlier'] == 0]['Profit'].values\n",
    "    # discount - outlier feature 1, profit - outlier feature 2\n",
    "    outliers_discount = out_df[out_df['Outlier'] == 1]['Discount'].values\n",
    "    outliers_profit = out_df[out_df['Outlier'] == 1]['Profit'].values\n",
    "\n",
    "    # Use threshold value to consider a datapoint inlier or outlier\n",
    "    # threshold = stats.scoreatpercentile(scores_pred,100 * outliers_fraction)\n",
    "    threshold = np.percentile(scores_pred, 100 * outliers_fraction)   \n",
    "    # decision function calculates the raw anomaly score for every point\n",
    "    Z = model.decision_function(np.c_[xx.ravel(), yy.ravel()]) * -1\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    # fill blue map colormap from minimum anomaly score to threshold value\n",
    "    ax_obj.contourf(xx, yy, Z, levels=np.linspace(Z.min(), threshold, 7),cmap=plt.cm.Blues_r)\n",
    "    # draw red contour line where anomaly score is equal to thresold\n",
    "    a = ax_obj.contour(xx, yy, Z, levels=[threshold],linewidths=2, colors='red')\n",
    "    # fill orange contour lines where range of anomaly score is from threshold to maximum anomaly score\n",
    "    ax_obj.contourf(xx, yy, Z, levels=[threshold, Z.max()],colors='orange')\n",
    "    b = ax_obj.scatter(inliers_discount, inliers_profit, c='white',s=20, edgecolor='k')\n",
    "    c = ax_obj.scatter(outliers_discount, outliers_profit, c='black',s=20, edgecolor='k')\n",
    "\n",
    "\n",
    "    ax_obj.legend([a.collections[0], b,c], ['learned decision function', 'inliers','outliers'],\n",
    "               prop=matplotlib.font_manager.FontProperties(size=10),loc='upper right')\n",
    "\n",
    "    ax_obj.set_xlim((0, 1))\n",
    "    ax_obj.set_ylim((0, 1))\n",
    "    ax_obj.set_xlabel('Discount')\n",
    "    ax_obj.set_ylabel('Sales')\n",
    "    ax_obj.set_title(subplot_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers_fraction = 0.01\n",
    "xx , yy = np.meshgrid(np.linspace(0, 1, 100), np.linspace(0, 1, 100))\n",
    "fig, ax = plt.subplots(1, 3, figsize=(20, 6))\n",
    "\n",
    "ax_objs = [ax[0], ax[1], ax[2]]\n",
    "models = [cblof_model, if_model, ae_model]\n",
    "plot_titles = ['Cluster-based Local Outlier Factor (CBLOF)',\n",
    "               'Isolation Forest',\n",
    "               'Auto-Encoder']\n",
    "\n",
    "for ax_obj, model, plot_title in zip(ax_objs, models, plot_titles):\n",
    "    visualize_anomalies(model=model,\n",
    "                        xx=xx, yy=yy,\n",
    "                        data_df=subset_df,\n",
    "                        ax_obj=ax_obj,\n",
    "                        subplot_title=plot_title)\n",
    "plt.axis('tight');\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
